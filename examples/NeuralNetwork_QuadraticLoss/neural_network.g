LoadPackage( "GradientBasedLearningForCAP" );

#! @Chapter Examples

#! @Section Neural Network with Quadratic Loss Function

#! This example demonstrates how to train a small feed-forward neural network
#! for a regression task using the $\texttt{GradientBasedLearningForCAP}$ package. We employ
#! the quadratic loss function and optimise the network parameters with
#! gradient descent.
#! The dataset consists of points $(x_1, x_2) \in \mathbb{R}^2$ with corresponding
#! outputs $y \in \mathbb{R}$ generated by a linear function with some added noise.
#! Concretely, the outputs are generated according to the formula
#! @BeginLatexOnly
#! \[
#! y = 2x_1 - 3x_2 + 1 + \text{some small random error}.
#! \]

#! @EndLatexOnly
#! @BeginLatexOnly
#! \begin{center}
#! \includegraphics[width=0.5\textwidth]{../examples/NeuralNetwork_QuadraticLoss/data/scatter_plot_training_examples.png}
#! \end{center}
#! @EndLatexOnly

#! We build a neural network with input dimension 2, no hidden layers, and output dimension 1.
#! Hence, the affine map between input and output layer has the following matrix dimensions (together with bias vector):
#! @BeginLatexOnly
#! \[
#! \binom{W_1}{b_1} \in \mathbb{R}^{ 3 \times 1 }.
#! \]
#! @EndLatexOnly
#! Where $W_1 \in \mathbb{R}^{2 \times 1}$ and $b_1 \in \mathbb{R}^1$ are the weights and bias to be learned.
#! Equivalently, the network computes for an input $a_0 \in \mathbb{R}^2$ the output
#! @BeginLatexOnly
#! \[
#! z_{1} := (a_0\;1)\binom{W_{1}}{b_{1}}=a_0 W_{1} + b_{1}\in \mathbb{R}^1.
#! \]
#! @EndLatexOnly
#! Hence, the number of parameters to learn is 3 (two weights and one bias).
#! We fit the neural network on the provided training examples for 30 epochs, and then compare the learned parameters
#! to the perfect weights used to generate the dataset.
#! We use the Adam optimiser for gradient descent. Hence, the initiat weights vector $(t, m_1, m_2, m_3, v_1, v_2, v_3, w_1, w_2, b_1) \in \mathbb{R}^{1+3+3+3}$
#! contains additional parameters for the optimiser (the $m$'s and $v$'s). We initialise $t$ to $1$ and $m$'s and $v$'s to $0$.


#! @Example
Smooth := SkeletalSmoothMaps;
#! SkeletalSmoothMaps
Lenses := CategoryOfLenses( Smooth );
#! CategoryOfLenses( SkeletalSmoothMaps )
Para := CategoryOfParametrisedMorphisms( Smooth );
#! CategoryOfParametrisedMorphisms( SkeletalSmoothMaps )
f := NeuralNetworkLossMorphism( Para, 2, [ ], 1, "IdFunc" );
#! ℝ^3 -> ℝ^1 defined by:
#! 
#! Underlying Object:
#! -----------------
#! ℝ^3
#! 
#! Underlying Morphism:
#! -------------------
#! ℝ^6 -> ℝ^1
optimizer := Lenses.AdamOptimizer();
#! function( n ) ... end
training_examples_path := Filename(
    DirectoriesPackageLibrary("GradientBasedLearningForCAP", "examples")[1],
    "NeuralNetwork_QuadraticLoss/data/training_examples.txt" );;
batch_size := 5;
#! 5
one_epoch_update := OneEpochUpdateLens( f, optimizer, 
                        training_examples_path, batch_size );
#! (ℝ^10, ℝ^10) -> (ℝ^1, ℝ^0) defined by:
#! 
#! Get Morphism:
#! ------------
#! ℝ^10 -> ℝ^1
#! 
#! Put Morphism:
#! ------------
#! ℝ^10 -> ℝ^10
w := [ 1, 0, 0, 0, 0, 0, 0, 0.21, -0.31, 0.7 ];
#! [ 1, 0, 0, 0, 0, 0, 0, 0.21, -0.31, 0.7 ]
nr_epochs := 30;
#! 30
w := Fit( one_epoch_update, nr_epochs, w );;
#! Epoch  0/30 - loss = 4.4574869198
#! Epoch  1/30 - loss = 1.0904439656285798
#! Epoch  2/30 - loss = 0.44893422753741707
#! Epoch  3/30 - loss = 0.24718222552679428
#! Epoch  4/30 - loss = 0.15816538314892969
#! Epoch  5/30 - loss = 0.11009214898573197
#! Epoch  6/30 - loss = 0.080765189573546586
#! Epoch  7/30 - loss = 0.061445427900729599
#! Epoch  8/30 - loss = 0.04803609207319106
#! Epoch  9/30 - loss = 0.038370239087861441
#! Epoch 10/30 - loss = 0.031199992288917108
#! Epoch 11/30 - loss = 0.025760084031019172
#! Epoch 12/30 - loss = 0.021557800050973547
#! Epoch 13/30 - loss = 0.018263315597330656
#! Epoch 14/30 - loss = 0.01564869258749324
#! Epoch 15/30 - loss = 0.013552162640841157
#! Epoch 16/30 - loss = 0.011856309185255345
#! Epoch 17/30 - loss = 0.010474254262187581
#! Epoch 18/30 - loss = 0.0093406409193010267
#! Epoch 19/30 - loss = 0.008405587711401704
#! Epoch 20/30 - loss = 0.0076305403249797375
#! Epoch 21/30 - loss = 0.0069853659369945552
#! Epoch 22/30 - loss = 0.0064462805409909937
#! Epoch 23/30 - loss = 0.0059943461353685126
#! Epoch 24/30 - loss = 0.0056143650058947617
#! Epoch 25/30 - loss = 0.0052940553411779294
#! Epoch 26/30 - loss = 0.0050234291867088457
#! Epoch 27/30 - loss = 0.0047943179297568897
#! Epoch 28/30 - loss = 0.0046000067074985669
#! Epoch 29/30 - loss = 0.004434950161766555
#! Epoch 30/30 - loss = 0.0042945495896027528
w;
#! [ 601, -0.00814765, -0.0328203, 0.00154532, 0.0208156, 0.0756998,
#! 0.047054, 2.01399, -2.9546, 0.989903 ]
#! @EndExample

#! We notice that the learned weights $w_1 \approx 2.01399$, $w_2 \approx -2.9546$, and $b_1 \approx 0.989903$ are close to the
#! perfect weights $2$, $-3$, and $1$ used to generate the dataset.
